1. Cross-domain Feature Selection for Language Identification
https://www.aclweb.org/anthology/I11-1062
The authors illustrated a new method in this paper to solve the problem of language of Identification which is usually considered as a solved problem. However, as suggested by the author, the performance of previous models always drop on cross-domain data which left space to refine the solution of this task. The idea of maximizing the difference between information gain for languages and that for domains for features selection based on former reliable works is innovative and conforms with the purpose of this work. Several plots and data showing the success in transfer learning make their project actually have a chance to be widely used in the real world. I believe languages are probably uniformly distributed within one dataset according to their description of all datasets, but I cannot find related information about how this model’s performance on the unknown(und) languages. It mentioned that totally 97 languages are trained in this case but obviously there are more languages in the world. Since other languages may not have enough sources to train, it leaves space for them to detect those low resources languages in fewer features.

2. Named Entity Recognition in Tweets: An Experimental Study
https://www.aclweb.org/anthology/D11-1141 
This paper provided a solution to several tasks including POS tag, chunking, and NER on twitter data. These tasks have several fairly good existing tools but suffer from a significant drop in performance when transferred to twitter data. The author considered several special characters of tweets like lots of infrequent entities, OOV and lack of background information within in short sentences and provided corresponding methods like clustering, T-CAP classifier, and distant supervision. Besides the impressive numbers in this work, I like the idea to consider different tasks separately so that using ‘the most’ suited models for each of them. That can be especially useful for starting in a relatively new domain or dataset. Even I believe the solution mentioned here could work, I still wonder if it can apply to twitter data we would collect today. We know that twitter languages are quickly changed overtime, considering this is a paper in 2011, it can be more convincible if it could survive in the reexamination it today.   

3. Demographic Dialectal Variation in Social Media: A Case Study of African-American English
https://www.aclweb.org/anthology/D16-1120
In this paper, the authors built probabilistic models to identify AAE dialect based on geoinformation and smartly evaluated by incorporating phonological and syntactic phenomena. Also, an improvement in existing language identification tools is achieved. I believe that capturing the assumption that there’s a correlation between demographics and languages lays a good foundation for their work. While they did not explain why using Gibbs sampling but I think it is a good model in this case since direct sampling is difficult. The contribution of their work is not limited to their insights on the analysis of special characteristics of dialects but with profound implication on social science. I will not surprise that if later works on other dialects of various languages in the world are established in a similar way or more corpus of those languages would be released. While the problem for today or other low-resource languages still exists that how we should explore the geoinformation when lots of tweets of some languages are not geo-enabled. 

4. PPDB: The Paraphrase Database
https://www.aclweb.org/anthology/N13-1092
While this database has already widely used, there are still improving space for it. The first thing is that proper nouns are still limited by the uneven distribution of the domain of corpus. If I search India, the word difference is in the list; If I search Ukraine, the word border is in the list. It’s possible that texts about Ukraine in the dataset always talk about the border problem. More corpus could possibly solve this problem to leverage unevenness. 
The second thing is that paraphrase pairs are not always substitutable. For example, if I search for nuts, the result list includes hazelnuts, walnuts, and groundnuts, etc. These specific kinds of nuts cannot replace nuts all the times. This may come from the limitation of this model since the features used in the ranking like n-gram, position, syntactic information can actually be very similar for these words. However, relations among these words are not discovered from that. I think it can be very difficult to find these relations based on the current model. Incorporating distant information from some knowledge bases may help.
Another thing I am wondering is how good this model will be if it applies to a gender-sensitive language. English does not need to deal with that for most cases but for language like French, the database could lose the information about gender if English is set as a foreign language. (Or probably we want the loss of gender information?)

5. A Word-Complexity Lexicon and A Neural Readability Ranking Model for Lexical Simplification
https://www.aclweb.org/anthology/D18-1410.pdf
In this paper, the authors designed a new neural readability ranking model which outperforming several tasks of lexicon simplification. It’s impressive to see one light model can improve on multiple tasks. The features designed are also convincing, for example, I think feeding syllables information to the model contribute to the robust of the model to the oov words since even words may never appear in training but the model has possibly already captured the morphology level structure that leads to the increment of the complexity of words. Two questions raised: first, even these tasks do not require a rank over entire vocabs, or in another way, these tasks only care about the relative complexity ranking of words under one target words, how is the transitivity of the ranking preserved by this model? And second, In SimplePPDB++ classification task, I notice that no-difference class has a longer range (0.8 ) than simplifying and complicating (0.6), how does the change of interval influence the result or how is the distribution of the system output? It seems like the authors give more possible space for no-difference.

6. Applying to Ph.D. Programs in Computer Science
https://www.cs.cmu.edu/~harchol/gradschooltalk.pdf
I cannot agree more with the point that research is very different from taking classes. At least for me, nothing will be too troublesome as long as you put effort to learn knowledge unless I’m learning quantum mechanics or literature. But research is different. When doing research, even there’s help from previous publications and advisors, there’s no guarantee I can make it on my tasks especially if I get involved in a new task. No one will tell me the correct solutions. In the beginning, sometimes I question myself, why are you not working this time? Well, I consider this as the beauty of science. I won’t think I am stupid if I’m not actually trying. Research allows me to make progress step by step, I will feel fine as long as I learn something and fix something each time I get something wrong. Getting accustomed to always get the correct answer is not the case for research. Enjoy the comfort I could get from discovering what I lack and want with the help of what people have once discovered or done and never be frustrated by knowing I am “stupid”, which actually origins from wading deeper into the area I am focusing on.
And thanks for telling me and I am happy to know that TOEFL is treated more seriously than GRE.

7. PAWS: Paraphrase Adversaries from Word Scrambling
https://arxiv.org/pdf/1904.01130.pdf
This work built a new dataset to address the problem of failing to distinguish pairs with only minor word order and syntactic structure differences, which, however, usually appear in real-world usage. Models like Bert and DIIN recover score numbers after training on this new dataset up to a point. This system relies on the quality or properties of NMT. While this can be ideal for this task since some NMT systems (Wu 2016) provide a good paraphrase function, it can also cause potential problems when later people want to create a similar database for other languages but have trouble finding another NMT system which can achieve good results on the translation of these languages and still maintain a similar paraphrase functionality as the NMT mentioned in this paper. (Considering Wu’s paper uses WMT dataset which focuses on EN-DE and EN-FR.) The reliability of generated sentence pairs should be guaranteed if we want to build dataset for different languages in the same pipeline.

8. Jacob's introduction to natural language processing is published. Reread several chapters.
https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf
After implementing NB 4 times in one semester, finally, there’s someone talking about the math/probability model behind it. For most time I just naively stopped at the conditional independent assumption of words in NB model, it’s surprising to know that multinomial distribution models the math for it. Well, I believe there’s no space for me to “criticize” these classical ML algorithms and they actually keep questioning ML researchers that whether you indeed create a neural model that works better than SVM or logistic regression. As tasks are getting more complex, it’s easy to not believe datasets of those tasks are (nearly) linear separable, but probably these traditional ML classifiers can be a strong baseline. I believe math reasoning or explanation is still very important for us to understand what we really learned through all these fancy models.

9. Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering
https://www.aclweb.org/anthology/C18-1328.pdf
I would say that’s the favorite paper I’ve read so far this semester since it indeed helped me review several good architectures. Although people nowadays have a belief in those big neural models, I think analyzing reasons or explanations behind these models, especially on multi-domain without carefully designing datasets can still help us to understand what these models actually learn. It’s not surprising to see reported numbers are often higher than reproduced numbers. I feel experiments can vary a lot if trained on different machines (as mentioned in the paper, the random seed could do so). I am quite wondering how the concept of transfer learning shifted after we step into the era of Elmo/Bert and etc. Fine-tuning on them seems enough for lots of tasks. But as most downstream tasks are still away from the usage in the real-world, are industries still prefer studying tasks independently for usage?

1. Cross-domain Feature Selection for Language Identification
https://www.aclweb.org/anthology/I11-1062
The authors illustrated a new method in this paper to solve the problem of language of Identification which is usually considered as a solved problem. However, as suggested by the author, the performance of previous models always drop on cross-domain data which left space to refine the solution of this task. The idea of maximizing the difference between information gain for languages and that for domains for features selection based on former reliable works is innovative and conforms with the purpose of this work. Several plots and data showing the success in transfer learning make their project actually have a chance to be widely used in the real world. I believe languages are probably uniformly distributed within one dataset according to their description of all datasets, but I cannot find related information about how this model’s performance on the unknown(und) languages. It mentioned that totally 97 languages are trained in this case but obviously there are more languages in the world. Since other languages may not have enough sources to train, it leaves space for them to detect those low resources languages in fewer features.

2. Named Entity Recognition in Tweets: An Experimental Study
https://www.aclweb.org/anthology/D11-1141 
This paper provided a solution to several tasks including POS tag, chunking, and NER on twitter data. These tasks have several fairly good existing tools but suffer from a significant drop in performance when transferred to twitter data. The author considered several special characters of tweets like lots of infrequent entities, OOV and lack of background information within in short sentences and provided corresponding methods like clustering, T-CAP classifier, and distant supervision. Besides the impressive numbers in this work, I like the idea to consider different tasks separately so that using ‘the most’ suited models for each of them. That can be especially useful for starting in a relatively new domain or dataset. Even I believe the solution mentioned here could work, I still wonder if it can apply to twitter data we would collect today. We know that twitter languages are quickly changed overtime, considering this is a paper in 2011, it can be more convincible if it could survive in the reexamination it today.   

3. Demographic Dialectal Variation in Social Media: A Case Study of African-American English
https://www.aclweb.org/anthology/D16-1120
In this paper, the authors built probabilistic models to identify AAE dialect based on geoinformation and smartly evaluated by incorporating phonological and syntactic phenomena. Also, an improvement in existing language identification tools is achieved. I believe that capturing the assumption that there’s a correlation between demographics and languages lays a good foundation for their work. While they did not explain why using Gibbs sampling but I think it is a good model in this case since direct sampling is difficult. The contribution of their work is not limited to their insights on the analysis of special characteristics of dialects but with profound implication on social science. I will not surprise that if later works on other dialects of various languages in the world are established in a similar way or more corpus of those languages would be released. While the problem for today or other low-resource languages still exists that how we should explore the geoinformation when lots of tweets of some languages are not geo-enabled. 

4. PPDB: The Paraphrase Database
https://www.aclweb.org/anthology/N13-1092
While this database has already widely used, there are still improving space for it. The first thing is that proper nouns are still limited by the uneven distribution of the domain of corpus. If I search India, the word difference is in the list; If I search Ukraine, the word border is in the list. It’s possible that texts about Ukraine in the dataset always talk about the border problem. More corpus could possibly solve this problem to leverage unevenness. 
The second thing is that paraphrase pairs are not always substitutable. For example, if I search for nuts, the result list includes hazelnuts, walnuts, and groundnuts, etc. These specific kinds of nuts cannot replace nuts all the times. This may come from the limitation of this model since the features used in the ranking like n-gram, position, syntactic information can actually be very similar for these words. However, relations among these words are not discovered from that. I think it can be very difficult to find these relations based on the current model. Incorporating distant information from some knowledge bases may help.
Another thing I am wondering is how good this model will be if it applies to a gender-sensitive language. English does not need to deal with that for most cases but for language like French, the database could lose the information about gender if English is set as a foreign language. (Or probably we want the loss of gender information?)

5. A Word-Complexity Lexicon and A Neural Readability Ranking Model for Lexical Simplification
https://www.aclweb.org/anthology/D18-1410.pdf
In this paper, the authors designed a new neural readability ranking model which outperforming several tasks of lexicon simplification. It’s impressive to see one light model can improve on multiple tasks. The features designed are also convincing, for example, I think feeding syllables information to the model contribute to the robust of the model to the oov words since even words may never appear in training but the model has possibly already captured the morphology level structure that leads to the increment of the complexity of words. Two questions raised: first, even these tasks do not require a rank over entire vocabs, or in another way, these tasks only care about the relative complexity ranking of words under one target words, how is the transitivity of the ranking preserved by this model? And second, In SimplePPDB++ classification task, I notice that no-difference class has a longer range (0.8 ) than simplifying and complicating (0.6), how does the change of interval influence the result or how is the distribution of the system output? It seems like the authors give more possible space for no-difference.

6. Applying to Ph.D. Programs in Computer Science
https://www.cs.cmu.edu/~harchol/gradschooltalk.pdf
I cannot agree more with the point that research is very different from taking classes. At least for me, nothing will be too troublesome as long as you put effort to learn knowledge unless I’m learning quantum mechanics or literature. But research is different. When doing research, even there’s help from previous publications and advisors, there’s no guarantee I can make it on my tasks especially if I get involved in a new task. No one will tell me the correct solutions. In the beginning, sometimes I question myself, why are you not working this time? Well, I consider this as the beauty of science. I won’t think I am stupid if I’m not actually trying. Research allows me to make progress step by step, I will feel fine as long as I learn something and fix something each time I get something wrong. Getting accustomed to always get the correct answer is not the case for research. Enjoy the comfort I could get from discovering what I lack and want with the help of what people have once discovered or done and never be frustrated by knowing I am “stupid”, which actually origins from wading deeper into the area I am focusing on.
And thanks for telling me and I am happy to know that TOEFL is treated more seriously than GRE.

7. PAWS: Paraphrase Adversaries from Word Scrambling
https://arxiv.org/pdf/1904.01130.pdf
This work built a new dataset to address the problem of failing to distinguish pairs with only minor word order and syntactic structure differences, which, however, usually appear in real-world usage. Models like Bert and DIIN recover score numbers after training on this new dataset up to a point. This system relies on the quality or properties of NMT. While this can be ideal for this task since some NMT systems (Wu 2016) provide a good paraphrase function, it can also cause potential problems when later people want to create a similar database for other languages but have trouble finding another NMT system which can achieve good results on the translation of these languages and still maintain a similar paraphrase functionality as the NMT mentioned in this paper. (Considering Wu’s paper uses WMT dataset which focuses on EN-DE and EN-FR.) The reliability of generated sentence pairs should be guaranteed if we want to build dataset for different languages in the same pipeline.

8. Jacob's introduction to natural language processing is published. Reread several chapters.
https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf
After implementing NB 4 times in one semester, finally, there’s someone talking about the math/probability model behind it. For most time I just naively stopped at the conditional independent assumption of words in NB model, it’s surprising to know that multinomial distribution models the math for it. Well, I believe there’s no space for me to “criticize” these classical ML algorithms and they actually keep questioning ML researchers that whether you indeed create a neural model that works better than SVM or logistic regression. As tasks are getting more complex, it’s easy to not believe datasets of those tasks are (nearly) linear separable, but probably these traditional ML classifiers can be a strong baseline. I believe math reasoning or explanation is still very important for us to understand what we really learned through all these fancy models.

9. Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering
https://www.aclweb.org/anthology/C18-1328.pdf
I would say that’s the favorite paper I’ve read so far this semester since it indeed helped me review several good architectures. Although people nowadays have a belief in those big neural models, I think analyzing reasons or explanations behind these models, especially on multi-domain without carefully designing datasets can still help us to understand what these models actually learn. It’s not surprising to see reported numbers are often higher than reproduced numbers. I feel experiments can vary a lot if trained on different machines (as mentioned in the paper, the random seed could do so). I am quite wondering how the concept of transfer learning shifted after we step into the era of Elmo/Bert and etc. Fine-tuning on them seems enough for lots of tasks. But as most downstream tasks are still away from the usage in the real-world, are industries still prefer studying tasks independently for usage?

10. Reread several chapters of Jacob's book.
https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf
This chapter provides a general view of how each step of simple neural network architecture will work. One point Jacob emphasized raised my notice is that at this point, the tradeoff between width and depth of neural networks is still not well understand. I’m quite interested in and pay attention to how math reasonings behind NN are being developed. I still remember the best paper of NerulIPs last year discussed the relation of ResNet and ODE. I believe eventually engineers will develop more suitable and efficient models for their tasks if more works like this are pushed. And that also reminds me that we need to keep track of theoretical papers as well. Learning early stopping from the textbook is important while it is also very beneficial to read cutting-edge findings, like professor Belkin’s new insight on widely used machine learning practice.

11. A Deep Reinforced Model for Abstractive Summarization
https://arxiv.org/pdf/1705.04304.pdf
That's one of my most favorite paper I've ever read this year: “a deep reinforced model for abstractive summarization”. Mainly for 2 reasons: First, it tackled the problem of the readability of past models by using two attention mechanisms and a mixed RL-ML objective function. Past models performed badly on long input and tended to output repeated sentences even they have good ROUGE scores. But if the readability cannot be guaranteed, why bother doing summarization? They also had a human evaluation of readability and relevance, probably for industrial usage. Second, they selected an appropriate RL model for this task. By my knowledge, RL is hard to train for NLP tasks due to its high variance. They selected a self-critical RL model to avoid learning an estimation of baseline and getting too close to the ground truth. Being able to transfer a model to succeed in vision tasks to NLP is impressive. One limitation or one point that they didn’t vindicate is that in the objective function, Lmixed=r*Lrl+(1-r)*Lml, the final r they choose was 0.9984, which means they took very little consideration of the maximum likelihood objective. So, that is to say, according to their evaluation, few considerations of ML objective gave huge improvement on readability. I want to do further evaluation of how r is influencing the readability in the same set-up of their work. Since they didn’t explain how r is selected, it’s better to start evaluation using numbers close to 0.9984, like uniformly taking ten numbers from [0.990,1]. And the second potential problem is that they directly never output the same trigram more than once during test since they found there’s no such a case in their dataset. That’s a too straight forward solution and fitting too much for one certain dataset. I will first see how’s the performance if this restriction is removed. Personally, I believed that won’t affect too much if their intra-temporal attention actually works as expected. If it dramatically influences the result, potential solutions still depend on the error cases. For example, if repeated trigrams are proper nouns like “natural language processing”, we can simply use abbreviation or pronouns to replace it.

12. Specializing Word Embeddings (for Parsing) by Information Bottleneck
https://www.aclweb.org/anthology/D19-1276.pdf
This is a very impressive project. It provided us a computation-efficient way, which is generally based on understandable concepts from information theory, to utilize pre-trained contextualized embeddings from Bert/Elmo for specific tasks. This paper can be more convincible if they evaluated their model on a different task and showed similar improvements. But they indeed mentioned this point in their paper so I believe this could be reasonably achieved. I can see the solid math foundations behind their team. It’s not very troublesome to understand reasons for the designed formula for encoders and decoder through their explanation, but thinking them as a whole from nothing requires a deep understanding of information theory and considerable math modeling experiences.

13. Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them
https://www.aclweb.org/anthology/N19-1061.pdf
"The Gonen and Goldberg paper highlighted a very important concept that biases still exist in word embeddings even after de-biasing efforts. They studied two techniques that both focus on a word's projection on gender direction of word embeddings. Figures 1 and 2 both support their claims that there is little movement in the word clusters and neighbors. The most interesting part of the paper for me was their “Classifying previously female- and male-biased words" results. It seemed throughout the paper that, the GN-Glove debiasing technique seemed to have more “success” in terms of removing bias for the downstream tasks. However, when it came to the classification task, the accuracy of the de-biased GN-Glove only dropped by about 2% whereas the Hard-Debiased accuracy dropped about 10% percent. Therefore, my question is why did the trend not follow here?"

14. A General-Purpose Tagger with Convolutional Neural Networks
https://www.aclweb.org/anthology/W17-4118.pdf
Based on my experience, for sequence labeling tasks, lstm is usually comparable with cnn architecture and I notice there are papers talking about how cnn is better. I somewhat think such kind of comparison is not totally fair. cnn sounds like it needs more complex work than LSTM. Different researchers give different cnn architecture and fine-tuned carefully, but lstm always has certain setups. And lstm can give strong baselines for lots of other tasks. One thing I always wonder is that, should we also consider the number of parameters for different models? I think that's especially necessary for simpler models. 